{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3TYUZg7hLbh"
      },
      "outputs": [],
      "source": [
        "#based on https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn import model_selection as  model_selection\n",
        "#load the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# to print the attributes of the dataset\n",
        "\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "\n",
        "#The dependent variable is median house value\n",
        "#Number of Instances: 20640\n",
        "\n",
        "#Number of Attributes: 8 numeric\n",
        "\n",
        "#Attribute Information:\n",
        "#       - MedInc        median income in block group\n",
        "#       - HouseAge      median house age in block group\n",
        "#       - AveRooms      average number of rooms per household\n",
        "#       - AveBedrms     average number of bedrooms per household\n",
        "#       - Population    block group population\n",
        "#       - AveOccup      average number of household members\n",
        "#       - Latitude      block group latitude\n",
        "#       - Longitude     block group longitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DwbEew4j-qp"
      },
      "outputs": [],
      "source": [
        "# print independet variabls (x)\n",
        "california_housing.data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_J4xT3vi1yb"
      },
      "outputs": [],
      "source": [
        "# print dependent variable (y)\n",
        "california_housing.target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v06P__Dk6AR"
      },
      "outputs": [],
      "source": [
        "california_housing.frame.info()\n",
        "#check there are any missing values (Here, in this data set there are no missing values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exrLEEHQlIqk"
      },
      "outputs": [],
      "source": [
        "#visualizing data\n",
        "\n",
        "california_housing.frame.hist(figsize=(12, 10), bins=30, edgecolor=\"black\")\n",
        "plt.subplots_adjust(hspace=0.7, wspace=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsKm_eUFm03Y"
      },
      "source": [
        "average rooms, average bedrooms, average occupation, and population, the range of the data are expanded in very short range. May be they are outliers. Further check on data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hi0kYQNmxr1"
      },
      "outputs": [],
      "source": [
        "features_of_interest = [\"AveRooms\", \"AveBedrms\", \"AveOccup\", \"Population\"]\n",
        "california_housing.frame[features_of_interest].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaacilpnPsP"
      },
      "source": [
        "Large gap on 75% of value and max value, possible outliers.\n",
        "Lets select one variable for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huhR5CZdnkQ6"
      },
      "outputs": [],
      "source": [
        "features_of_interest = [\"AveRooms\"]\n",
        "california_housing.frame[features_of_interest].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZVsLdLon7LD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the  dataset\n",
        "data_X, data_y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# select the \"AveRooms\" data\n",
        "data_X=data_X[:,2]\n",
        "\n",
        "#save index of the maximum value, will be used later\n",
        "index_max=np.argmax(data_X)\n",
        "max_x=data_X[index_max]\n",
        "max_y=data_y[index_max]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUHV1JGIpuhP"
      },
      "outputs": [],
      "source": [
        "#verification that we have selected correct set\n",
        "print(np.max(data_X),np.min(data_X),np.mean(data_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0omiS6emqwJ8"
      },
      "outputs": [],
      "source": [
        "#we have lot of data samples, just select 100 of them\n",
        "data_X=data_X[100:200]\n",
        "data_y=data_y[100:200]\n",
        "#remember index starts from 0\n",
        "# Split the data into training/testing sets\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(data_X, data_y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val  = model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"Total dataset elements\",data_X.shape)\n",
        "print(\"Train dataset elements\",X_train.shape)\n",
        "print(\"Validatiom dataset elements\",X_val.shape)\n",
        "print(\"Test dataset elements\",X_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EH4V2PKv1Z3"
      },
      "outputs": [],
      "source": [
        "#rashape data as vectors\n",
        "X_train = X_train.reshape(X_train.shape[0], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 1)\n",
        "y_train = y_train.reshape(y_train.shape[0], 1)\n",
        "y_test = y_test.reshape(y_test.shape[0], 1)\n",
        "yval = y_val.reshape(y_val.shape[0], 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjKFFZSkuT9l"
      },
      "outputs": [],
      "source": [
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# The intercept\n",
        "print(\"Intercept w_0: \\n\", regr.intercept_)\n",
        "# The coefficients\n",
        "print(\"Coefficient w_1: \\n\", regr.coef_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using inv(X^T X) X^Ty"
      ],
      "metadata": {
        "id": "ookAhy2qQyfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Reshape y_train to ensure it's a column vector\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "# Add a column of 1s to X_train for the intercept term\n",
        "intercept = np.ones((X_train.shape[0], 1))\n",
        "X_train_with_intercept = np.hstack((intercept, X_train))\n",
        "\n",
        "# Calculate beta_hat using the normal equation\n",
        "X_transpose = X_train_with_intercept.T\n",
        "X_transpose_X = np.dot(X_transpose, X_train_with_intercept)\n",
        "X_transpose_X_inv = np.linalg.inv(X_transpose_X)\n",
        "X_transpose_y = np.dot(X_transpose, y_train)\n",
        "beta_hat = np.dot(X_transpose_X_inv, X_transpose_y)\n",
        "\n",
        "# Extract the intercept and coefficients\n",
        "intercept_hat = beta_hat[0]\n",
        "coefficients = beta_hat[1:]\n",
        "\n",
        "print(\"Intercept w_0 :\", intercept_hat)\n",
        "print(\"Coefficient w_1:\", coefficients)\n"
      ],
      "metadata": {
        "id": "vuvUbFZCRkgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqTuNpL7OnrK"
      },
      "outputs": [],
      "source": [
        "yhat=regr.predict(X_train)\n",
        "\n",
        "RSS = np.sum((yhat - y_train)**2)\n",
        "print('RSS=', RSS)\n",
        "N=len(y_train)\n",
        "print(N)\n",
        "RSE = np.sqrt(1/(N-2)*RSS)\n",
        "print('RSE=', RSE)\n",
        "TSS = np.sum((y_train- np.mean(y_train))**2)\n",
        "print('TSS=', TSS)\n",
        "# Residual Sum of Sqaures (RSS)\n",
        "RSS = np.sum((yhat - y_train)**2)\n",
        "# Residual Standard Error (RSE)\n",
        "\n",
        "RSE = np.sqrt(1/(N-2)*RSS)\n",
        "# Total Sum of Squares (TSS)\n",
        "TSS = np.sum((y_train - np.mean(y_train))**2)\n",
        "R2 = (TSS - RSS)/TSS\n",
        "print('R2=', R2)\n",
        "# Calculation of R2 using sklearn\n",
        "R2 = regr.score(X_train,y_train)\n",
        "\n",
        "print('R2 =', R2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJwFNMINOnrK"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import t\n",
        "\n",
        "\n",
        "w_1=regr.coef_\n",
        "w_0=regr.intercept_\n",
        "sigma2 = np.var(y_train - yhat)\n",
        "SE2w0 =  sigma2*(1/N + np.mean(X_train)**2/np.sum((X_train - np.mean(X_train))**2))\n",
        "SE2w1 = sigma2/np.sum((X_train - np.mean(X_train))**2)\n",
        "tw1 = (w_1 - 0)/np.sqrt(SE2w1)\n",
        "tw0 = (w_0 - 0)/np.sqrt(SE2w0)\n",
        "\n",
        "print('Standard errors for intecept and w1: ', SE2w0, SE2w1)\n",
        "print('t-statistic for intecept and w1: ', tw0, tw1)\n",
        "pw1 = t.sf(np.abs(tw1), N-2)\n",
        "\n",
        "pw0 = t.sf(np.abs(tw0), N-2)\n",
        "print('pvale  for intecept and w1: ', pw0, pw1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5jsKUND4USP"
      },
      "outputs": [],
      "source": [
        "# Make predictions using the testing set\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test, y_test, color=\"black\", label='Data points')\n",
        "plt.plot(X_test, y_pred, color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Testing data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPMmrNwB3Kr3"
      },
      "outputs": [],
      "source": [
        "#Impact of outliers\n",
        "# load maximum x value and corresponding y value to the training dataset\n",
        "X_train[59,:]=max_x\n",
        "y_train[59,:]=max_y\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "print(\"intercept: \\n\", regr.intercept_)\n",
        "\n",
        "#see how training line look like\n",
        "\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_train, y_train, color=\"black\", )\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2)\n",
        "\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.title(\"Training data\")\n",
        "plt.show()\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Training data (zoom)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "# The intercept\n",
        "print(\"Coefficients: \\n\", regr.intercept_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcu-XhZR6QYc"
      },
      "outputs": [],
      "source": [
        "# see the predictions using the testing set, where modelled trained with an outlier\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test, y_test, color=\"black\", label='Data points')\n",
        "plt.plot(X_test, y_pred, color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Testing data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ1gZdtvByub"
      },
      "outputs": [],
      "source": [
        "# RANSAC (RANdom SAmple Consensus)\n",
        "# Train the model using the training sets\n",
        "ransac = linear_model.RANSACRegressor()\n",
        "#The RANSAC regressor automatically splits the data into inliers and outliers, and the fitted line is determined only by the identified inliers.\n",
        "\n",
        "\n",
        "ransac.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot outputs\n",
        "\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "plt.plot(X_train, ransac.predict(X_train), color=\"red\", linewidth=2, label=r'RANSAC Regressor')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.title(\"Training data\")\n",
        "plt.show()\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "plt.plot(X_train, ransac.predict(X_train), color=\"red\", linewidth=2, label=r'RANSAC Regressor')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Training data (zoom)\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwUGd3NWEzUE"
      },
      "outputs": [],
      "source": [
        "line_y_ransac = ransac.predict(X_test)\n",
        "# Plot outputs\n",
        "plt.scatter(X_test, y_test, color=\"black\", label='Data points')\n",
        "plt.plot(X_test, y_pred, color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "plt.plot(X_test, line_y_ransac, color=\"red\", linewidth=2, label=r'RANSAC Regressor')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Testing data\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqcArWDAOnrP"
      },
      "source": [
        "Polynomial Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snA33xW2OnrP"
      },
      "outputs": [],
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    linear_regression = LinearRegression()\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
        "        degrees[i], -scores.mean(), scores.std()))\n",
        "\n",
        "# plt.savefig('./figures/polynomial_regresion.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FOH0uzROnrQ"
      },
      "source": [
        "Polynomial Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2VCvdd7OnrQ"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "alpha = 0.0001 # lambda in the slides\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    ridge = linear_model.Ridge(alpha=alpha, fit_intercept=True)\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"ridge_regresssion\", ridge)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
        "        degrees[i], -scores.mean(), scores.std()))\n",
        "\n",
        "# plt.savefig('./figures/polynomial_ridge_regresion.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YedZ8kMGOnrQ"
      },
      "source": [
        "Ridge Coefficients as a Function of the Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmou9szoOnrQ"
      },
      "outputs": [],
      "source": [
        "# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n",
        "# License: BSD 3 clause\n",
        "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "\n",
        "# X is the 10x10 Hilbert matrix\n",
        "X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\n",
        "y = np.ones(10)\n",
        "\n",
        "# #############################################################################\n",
        "# Compute paths\n",
        "\n",
        "n_alphas = 200\n",
        "alphas = np.logspace(-10, -2, n_alphas)\n",
        "\n",
        "coefs = []\n",
        "for a in alphas:\n",
        "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
        "    ridge.fit(X, y)\n",
        "    coefs.append(ridge.coef_)\n",
        "\n",
        "# #############################################################################\n",
        "# Display results\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.plot(alphas, coefs)\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
        "plt.xlabel('$\\lambda$')\n",
        "plt.ylabel('weights')\n",
        "plt.title('Ridge coefficients as a function of the regularization')\n",
        "plt.axis('tight')\n",
        "# plt.savefig('./figures/redge_coeff_regularization.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS8j4V3LOnrR"
      },
      "source": [
        "LASSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP65q-KjOnrR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data with informative and irrelevant features\n",
        "X, y, true_coefficients = make_regression(\n",
        "    n_samples=100, n_features=50, n_informative=10, noise=20.5, coef=True, random_state=42\n",
        ")\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_model = Lasso(alpha=3)  # Alpha is the regularization strength\n",
        "\n",
        "# Fit the Lasso regression model to the data\n",
        "lasso_model.fit(X, y)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Fit the Linear Regression model to the data\n",
        "linear_model.fit(X, y)\n",
        "\n",
        "# Plot the true coefficients and the estimated coefficients for Lasso and Linear Regression\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(true_coefficients, label='True Coefficients', marker='o')\n",
        "plt.plot(lasso_model.coef_, label='Lasso Regression', marker='x')\n",
        "plt.plot(linear_model.coef_, label='Linear Regression', marker='s')\n",
        "\n",
        "plt.xlabel('Coefficient Index')\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('True vs. Estimated Coefficients')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of non-zero coefficients (Linear Regression):\", np.sum(linear_model.coef_ != 0))\n",
        "print(\"Number of non-zero coefficients (Lasso Regression):\", np.sum(lasso_model.coef_ != 0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNDWwfOwOnrR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the true coefficients as stem plot\n",
        "plt.stem(range(len(true_coefficients)), true_coefficients, markerfmt='go', linefmt='g', basefmt=' ')\n",
        "\n",
        "# Plot the Lasso coefficients as stem plot with a small offset\n",
        "plt.stem(np.arange(len(true_coefficients)) + 0.3, lasso_model.coef_, markerfmt='rx', linefmt='r', basefmt=' ')\n",
        "\n",
        "# Plot the Linear Regression coefficients as stem plot with a larger offset\n",
        "plt.stem(np.arange(len(true_coefficients)) + 0.5, linear_model.coef_, markerfmt='bs', linefmt='b', basefmt=' ')\n",
        "\n",
        "plt.xlabel('Coefficient Index')\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('True vs. Estimated Coefficients')\n",
        "plt.legend(['True Coefficients', 'Lasso Regression', 'Linear Regression'])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geSMXVsXOnrS"
      },
      "source": [
        "Multi Variable Linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iaQXkG8OnrS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn import model_selection as  model_selection\n",
        "#load the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load the diabetes dataset\n",
        "data_X, data_y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "#remember index starts from 0\n",
        "# Split the data into training/testing sets\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(data_X, data_y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val  = model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"Total dataset elements\",data_X.shape)\n",
        "print(\"Train dataset elements\",X_train.shape)\n",
        "print(\"Validatiom dataset elements\",X_val.shape)\n",
        "print(\"Test dataset elements\",X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx2VOvH_OnrT"
      },
      "outputs": [],
      "source": [
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "# The intercept\n",
        "print(\"Coefficients: \\n\", regr.intercept_)\n",
        "\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "#see the impact of each variable\n",
        "\n",
        "regr_coef=regr.coef_/np.max(regr.coef_)\n",
        "print(\"Coefficients: \\n\", regr_coef)\n",
        "#Here, value=1 is the most relevant feature.\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "california_housing.data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyE4wRHCOnrT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming you have already fitted the linear regression model (regr.fit(X_train, y_train))\n",
        "\n",
        "# Add a constant column to the feature matrix (required by statsmodels)\n",
        "X_train_with_constant = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the OLS (Ordinary Least Squares) model\n",
        "ols_model = sm.OLS(y_train, X_train_with_constant).fit()\n",
        "\n",
        "# Get summary statistics of the model\n",
        "summary = ols_model.summary()\n",
        "\n",
        "# Extract p-values from the summary for all features\n",
        "p_values = summary.tables[1].data[1:]\n",
        "\n",
        "# Create a DataFrame to associate p-values with feature names\n",
        "p_values_df = pd.DataFrame(p_values, columns=['Feature', 'Coefficient', 'Standard Error', 't-value', 'P-Value', 'Lower CI', 'Upper CI'])\n",
        "p_values_df['P-Value'] = p_values_df['P-Value'].astype(float)\n",
        "\n",
        "\n",
        "print(summary)\n",
        "print(p_values_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MI4gFQ0OnrT"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "lm = linear_model.LinearRegression()\n",
        "lm.fit(X_train,y_train)\n",
        "params = np.append(lm.intercept_,lm.coef_)\n",
        "predictions = lm.predict(X_train)\n",
        "\n",
        "newX = pd.DataFrame({\"Constant\":np.ones(len(X_train))}).join(pd.DataFrame(X_train))\n",
        "MSE = (sum((y_train-predictions)**2))/(len(newX)-len(newX.columns))\n",
        "\n",
        "# Note if you don't want to use a DataFrame replace the two lines above with\n",
        "# newX = np.append(np.ones((len(X),1)), X, axis=1)\n",
        "# MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))\n",
        "\n",
        "var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
        "sd_b = np.sqrt(var_b)\n",
        "ts_b = params/ sd_b\n",
        "\n",
        "\n",
        "\n",
        "sd_b = np.round(sd_b,3)\n",
        "ts_b = np.round(ts_b,3)\n",
        "\n",
        "params = np.round(params,4)\n",
        "\n",
        "myDF3 = pd.DataFrame()\n",
        "myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"] = [params,sd_b,ts_b]\n",
        "print(myDF3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}